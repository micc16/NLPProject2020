{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"Development_data/set1/a1.txt\"\n",
    "datafile2 = \"Development_data/set1/a2.txt\"\n",
    "questionfile = \"Development_data/set1/a1q.txt\"\n",
    "\n",
    "data = open(datafile, \"r\")\n",
    "data2 = open(datafile2, \"r\")\n",
    "questions = open(questionfile, \"r\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = data.read()\n",
    "sample_txt_sentences = nltk.sent_tokenize(sample_txt)\n",
    "\n",
    "questions_txt = questions.read()\n",
    "questions_txt_sentences = nltk.sent_tokenize(questions_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_in_txt_by_question(text_sentences_list, question_sentence):\n",
    "    \n",
    "    q_tokenize = nltk.word_tokenize(question_sentence)\n",
    "    q_bigrams = list(ngrams(q_tokenize,2))\n",
    "    q_trigrams = list(ngrams(q_tokenize,3))\n",
    "    \n",
    "    #print(q_bigrams)\n",
    "    \n",
    "    best_sentence = \"\"\n",
    "    best_num_matching = -1\n",
    "    #print(text_sentences_list)\n",
    "    for sentence in text_sentences_list:\n",
    "        txt_tokenize = nltk.word_tokenize(sentence)\n",
    "        txt_bigrams = list(ngrams(txt_tokenize,2))\n",
    "        txt_trigrams = list(ngrams(txt_tokenize,3))\n",
    "        \n",
    "        curr_matching = 0\n",
    "        sent_words = sentence.split()\n",
    "        for bigram in txt_bigrams:\n",
    "            if bigram in q_bigrams:\n",
    "                curr_matching += 1\n",
    "                #print(curr_matching)\n",
    "        for trigram in txt_trigrams:\n",
    "            if trigram in q_trigrams:\n",
    "                curr_matching += 1\n",
    "        if curr_matching > best_num_matching:\n",
    "            best_sentence = sentence\n",
    "            best_num_matching = curr_matching\n",
    "            \n",
    "    return best_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Under King Djoser, the first king of the Third Dynasty of the Old Kingdom, the royal capital of Egypt was moved to Memphis, where Djoser established his court.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sentence_in_txt_by_question(sample_txt_sentences, questions_txt_sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was the first king of the Third Dynasty of the Old Kingdom?\n"
     ]
    }
   ],
   "source": [
    "print(questions_txt_sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    named_ent_list = []\n",
    "    res = []\n",
    "    question_ne_list = []\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "    named_ent = nltk.ne_chunk(tagged)\n",
    "        \n",
    "    #get named entities\n",
    "    for chunk in named_ent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            question_ne_list.append((chunk.label(), ' '.join(c[0] for c in chunk)))\n",
    "    \n",
    "    #get full output\n",
    "    named_ent_list += question_ne_list\n",
    "    res = (str(named_ent).split(\"\\n  \"))\n",
    "    \n",
    "    #print((named_ent_list, res))\n",
    "    return (named_ent_list, res)\n",
    "        \n",
    "    #return named_ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('ORGANIZATION', 'Old Kingdom')],\n",
       " ['(S',\n",
       "  'Who/WP',\n",
       "  'was/VBD',\n",
       "  'the/DT',\n",
       "  'first/JJ',\n",
       "  'king/NN',\n",
       "  'of/IN',\n",
       "  'the/DT',\n",
       "  'Third/NNP',\n",
       "  'Dynasty/NNP',\n",
       "  'of/IN',\n",
       "  'the/DT',\n",
       "  '(ORGANIZATION Old/NNP Kingdom/NNP)',\n",
       "  '?/.)'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_sentence(questions_txt_sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('ORGANIZATION', 'Old Kingdom'),\n",
       "  ('GPE', 'Egypt'),\n",
       "  ('GPE', 'Memphis'),\n",
       "  ('PERSON', 'Djoser')],\n",
       " ['(S',\n",
       "  'Under/IN',\n",
       "  'King/NNP',\n",
       "  'Djoser/NNP',\n",
       "  ',/,',\n",
       "  'the/DT',\n",
       "  'first/JJ',\n",
       "  'king/NN',\n",
       "  'of/IN',\n",
       "  'the/DT',\n",
       "  'Third/NNP',\n",
       "  'Dynasty/NNP',\n",
       "  'of/IN',\n",
       "  'the/DT',\n",
       "  '(ORGANIZATION Old/NNP Kingdom/NNP)',\n",
       "  ',/,',\n",
       "  'the/DT',\n",
       "  'royal/JJ',\n",
       "  'capital/NN',\n",
       "  'of/IN',\n",
       "  '(GPE Egypt/NNP)',\n",
       "  'was/VBD',\n",
       "  'moved/VBN',\n",
       "  'to/TO',\n",
       "  '(GPE Memphis/NNP)',\n",
       "  ',/,',\n",
       "  'where/WRB',\n",
       "  '(PERSON Djoser/NNP)',\n",
       "  'established/VBD',\n",
       "  'his/PRP$',\n",
       "  'court/NN',\n",
       "  './.)'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_answer = find_sentence_in_txt_by_question(sample_txt_sentences, questions_txt_sentences[3])\n",
    "process_sentence(txt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
